# --- IMPORTS ---
# These lines bring in the tools we need from other libraries.

# FastAPI is the framework for building the API. 
# UploadFile and File are used to handle file uploads from users.
# Form is used to handle simple text inputs sent alongside files.
from fastapi import FastAPI, UploadFile, File, Form

# CORSMiddleware is a security feature. It allows your frontend (website) 
# to talk to this backend even if they are on different "ports" (e.g., localhost:3000 vs localhost:8000).
from fastapi.middleware.cors import CORSMiddleware

# StreamingResponse is a special way to return data that lets the user download a file.
from fastapi.responses import StreamingResponse

# Pandas is the standard tool for data analysis. We alias it as 'pd' for valid brevity.
import pandas as pd

# NumPy is used for mathematical operations (like NaN/missing values). Aliased as 'np'.
import numpy as np

# 'io' helps us handle input/output streams. We use it to trick Pandas into reading a file from memory instead of the hard drive.
import io

# Statsmodels is used for advanced statistics (like the Regression analysis).
import statsmodels.api as sm

# --- APP SETUP ---

# Initialize the application. This 'app' variable is what runs the server.
app = FastAPI()

# Add "Middleware". This runs before every request.
# Here, we are configuring CORS (Cross-Origin Resource Sharing).
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # "*" means allow ANY website to connect to this API.
    allow_credentials=True, # Allows cookies/auth headers.
    allow_methods=["*"],    # Allows all HTTP methods (GET, POST, PUT, DELETE).
    allow_headers=["*"],    # Allows all types of headers.
)

# --- ROUTES (ENDPOINTS) ---

# @app.get tells FastAPI: "When a user goes to the URL '/', run this function."
# This is a basic health check to see if the server is running.
@app.get("/")
def root():
    return {"status": "ok"}

# @app.post tells FastAPI: "When a user sends data (POST) to '/analyze', run this."
# 'async def' allows the server to multitask while waiting for the file to upload.
@app.post("/analyze")
async def analyze(file: UploadFile = File(...)):
    # 1. READ THE FILE
    # 'await' pauses this function until the file is fully read into memory.
    content = await file.read()
    
    # io.BytesIO turns the raw bytes into a file-like object so Pandas can read it like a normal CSV.
    df = pd.read_csv(io.BytesIO(content))
    
    # 2. CALCULATE BASIC STATISTICS
    # Create a dictionary to store our results.
    result = {
        # .shape returns (rows, columns). We grab index 0 for rows, 1 for cols.
        "shape": {"rows": int(df.shape[0]), "cols": int(df.shape[1])},
        
        # .columns.tolist() gives us a list of all column names ["Age", "Salary", etc.]
        "columns": df.columns.tolist(),
        
        # .isna() checks for missing values (True/False).
        # .sum() counts the "True" values.
        # .to_dict() converts it to a format JSON can understand.
        "missing_by_column": df.isna().sum().astype(int).to_dict(),
    }
    
    # 3. STATISTICS FOR NUMBERS
    # We only want to calculate mean/min/max for number columns, not text.
    numeric = df.select_dtypes(include="number")
    
    # If the dataframe actually has numbers...
    if not numeric.empty:
        # .describe() calculates mean, std, min, max, etc. automatically.
        result["describe"] = numeric.describe().to_dict()
        # .corr() calculates how related columns are (Correlation Matrix).
        result["corr"] = numeric.corr(numeric_only=True).to_dict()
    
    # Return the dictionary. FastAPI automatically converts this to JSON for the frontend.
    return result

# This endpoint performs Linear Regression (predicting Y based on X).
@app.post("/regress")
async def regress(
    file: UploadFile = File(...),   # The CSV file
    x_cols: list[str] = Form(...),  # A list of column names to use as predictors (Independent variables)
    y_col: str = Form(...)          # The single column we want to predict (Dependent variable)
):
    # Read the file into a Pandas DataFrame
    content = await file.read()
    df = pd.read_csv(io.BytesIO(content))
    
    # --- ERROR CHECKING ---
    # Check if the user actually sent X columns
    if not x_cols or len(x_cols) == 0:
        return {"error": "At least one X column required"}
    
    # Check if the columns actually exist in the CSV
    for col in x_cols:
        if col not in df.columns:
            return {"error": f"Column {col} not found"}
    
    if y_col not in df.columns:
        return {"error": f"Y column {y_col} not found"}
    
    # --- PREPARE DATA ---
    # Create a sub-dataframe with only the columns we need.
    # .dropna() removes any row that has a missing value in these specific columns.
    # Statsmodels will crash if there are missing values (NaN).
    sub = df[x_cols + [y_col]].dropna()
    
    if len(sub) == 0:
        return {"error": "No valid data points after removing missing values"}
    
    # Define X (predictors) and y (target).
    X = sub[x_cols]
    
    # IMPORTANT: Statsmodels does not add an intercept (constant) by default.
    # We must add a column of 1s so the line doesn't be forced to go through (0,0).
    X = sm.add_constant(X)
    y = sub[y_col]
    
    # --- RUN REGRESSION ---
    # OLS = Ordinary Least Squares (Standard Linear Regression).
    # .fit() actually performs the math to find the best line.
    model = sm.OLS(y, X).fit()
    
    # get_influence() calculates advanced stats to find outliers (weird data points).
    influence = model.get_influence()
    
    # --- RETURN RESULTS ---
    # We extract all the specific numbers from the 'model' object.
    return {
        "n": int(len(sub)), # Sample size
        "x_cols": x_cols,
        "y_col": y_col,
        "intercept": float(model.params["const"]), # The Y-intercept
        "coefficients": {col: float(model.params[col]) for col in x_cols}, # The slopes
        "p_values": {col: float(model.pvalues[col]) for col in x_cols}, # Statistical significance
        "r2": float(model.rsquared), # How well the model fits (0 to 1)
        "r2_adj": float(model.rsquared_adj), # Adjusted R2
        "f_statistic": float(model.fvalue),
        "f_pvalue": float(model.f_pvalue),
        "y": y.tolist(), # The actual data
        "residuals": model.resid.tolist(), # The errors (Actual - Predicted)
        "fitted": model.fittedvalues.tolist(), # The predictions
        "standardized_residuals": influence.resid_studentized_internal.tolist(), # Normalized errors
        "leverage": influence.hat_matrix_diag.tolist(), # How much influence a point has
        "cooks_distance": influence.cooks_distance[0].tolist(), # Combined outlier score
    }

# This endpoint cleans data based on user options (checkboxes).
@app.post("/clean")
async def clean_data(
    file: UploadFile = File(...),
    # By default, all these options are "false". The user sets them to "true" in the frontend.
    drop_na: str = Form("false"),
    fill_mean: str = Form("false"),
    fill_median: str = Form("false"),
    remove_outliers_iqr: str = Form("false"),
    remove_outliers_zscore: str = Form("false"),
    drop_duplicates: str = Form("false"),
    drop_high_missing: str = Form("false"),
    missing_threshold: str = Form("50.0")
):
    # Read the file
    content = await file.read()
    df = pd.read_csv(io.BytesIO(content))
    
    # --- CONVERT INPUTS ---
    # Data from 'Form' always comes as strings. We convert "true"/"false" strings to actual Booleans.
    drop_na = drop_na.lower() == "true"
    fill_mean = fill_mean.lower() == "true"
    fill_median = fill_median.lower() == "true"
    remove_outliers_iqr = remove_outliers_iqr.lower() == "true"
    remove_outliers_zscore = remove_outliers_zscore.lower() == "true"
    drop_duplicates = drop_duplicates.lower() == "true"
    drop_high_missing = drop_high_missing.lower() == "true"
    missing_threshold = float(missing_threshold)
    
    original_shape = df.shape
    operations = [] # We will keep a log of what we did to the data.
    
    # --- CLEANING LOGIC ---
    
    # 1. DROP COLUMNS with too many missing values
    if drop_high_missing:
        threshold = missing_threshold / 100.0
        # Calculate percentage of missing values per column
        missing_pct = df.isna().mean()
        # Find columns where missing % > threshold
        cols_to_drop = missing_pct[missing_pct > threshold].index.tolist()
        if cols_to_drop:
            df = df.drop(columns=cols_to_drop)
            operations.append(f"Dropped {len(cols_to_drop)} columns with >{missing_threshold}% missing")
    
    # 2. FILL MISSING WITH MEAN
    if fill_mean:
        numeric_cols = df.select_dtypes(include=["number"]).columns
        filled = 0
        for col in numeric_cols:
            missing_count = int(df[col].isna().sum())
            if missing_count > 0:
                mean_val = df[col].mean()
                # .fillna() replaces NaNs with the value we provide
                df[col] = df[col].fillna(mean_val)
                filled += missing_count
        if filled > 0:
            operations.append(f"Filled {filled} missing numeric values with mean")
    
    # 3. FILL MISSING WITH MEDIAN
    if fill_median:
        numeric_cols = df.select_dtypes(include=["number"]).columns
        filled = 0
        for col in numeric_cols:
            missing_count = int(df[col].isna().sum())
            if missing_count > 0:
                median_val = df[col].median()
                df[col] = df[col].fillna(median_val)
                filled += missing_count
        if filled > 0:
            operations.append(f"Filled {filled} missing numeric values with median")
    
    # 4. DROP ROWS with missing values (if any are left)
    if drop_na:
        before = len(df)
        df = df.dropna()
        dropped = before - len(df)
        if dropped > 0:
            operations.append(f"Dropped {dropped} rows with missing values")
    
    # 5. REMOVE OUTLIERS (IQR Method - Interquartile Range)
    # This removes data that is statistically "too far" from the middle 50% of data.
    if remove_outliers_iqr:
        numeric_cols = df.select_dtypes(include=["number"]).columns
        before = len(df)
        for col in numeric_cols:
            Q1 = df[col].quantile(0.25) # 25th percentile
            Q3 = df[col].quantile(0.75) # 75th percentile
            IQR = Q3 - Q1
            lower = Q1 - 1.5 * IQR
            upper = Q3 + 1.5 * IQR
            # Keep only rows inside the lower/upper bounds
            df = df[(df[col] >= lower) & (df[col] <= upper)]
        removed = before - len(df)
        if removed > 0:
            operations.append(f"Removed {removed} outlier rows (IQR method)")
    
    # 6. REMOVE OUTLIERS (Z-Score Method)
    # This removes data that is more than 3 standard deviations from the mean.
    if remove_outliers_zscore:
        numeric_cols = df.select_dtypes(include=["number"]).columns
        before = len(df)
        for col in numeric_cols:
            # Calculate Z-score for every value
            z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())
            # Keep rows where Z-score is less than 3
            df = df[z_scores < 3]
        removed = before - len(df)
        if removed > 0:
            operations.append(f"Removed {removed} outlier rows (Z-score method)")
    
    # 7. DROP DUPLICATES
    if drop_duplicates:
        before = len(df)
        df = df.drop_duplicates()
        removed = before - len(df)
        if removed > 0:
            operations.append(f"Removed {removed} duplicate rows")
    
    # --- RETURN FILE ---
    # We need to turn the DataFrame back into a CSV file to send to the user.
    output = io.StringIO()
    df.to_csv(output, index=False) # Write to the string buffer
    output.seek(0) # Reset the "cursor" to the start of the file so it can be read
    
    # StreamingResponse sends the file back to the browser as a download.
    return StreamingResponse(
        io.BytesIO(output.getvalue().encode()),
        media_type="text/csv",
        headers={
            "Content-Disposition": "attachment; filename=cleaned_data.csv",
            # We send custom headers so the frontend knows what operations happened
            "X-Operations": "; ".join(operations) if operations else "No operations applied",
            "X-Original-Shape": f"{original_shape[0]}x{original_shape[1]}",
            "X-New-Shape": f"{df.shape[0]}x{df.shape[1]}"
        }
    )

# This endpoint handles "Pagination". It sends data in small chunks (pages)
# so the browser doesn't crash trying to load 100,000 rows at once.
@app.post("/get_data")
async def get_data(
    file: UploadFile = File(...),
    page: str = Form("0"),       # Current page number (starts at 0)
    page_size: str = Form("20")  # How many rows per page
):
    """Get paginated data for editing"""
    try:
        content = await file.read()
        df = pd.read_csv(io.BytesIO(content))
        
        # Convert inputs to integers
        page = int(page)
        page_size = int(page_size)
        
        # Calculate which rows to slice
        total_rows = len(df)
        start = page * page_size
        end = start + page_size
        
        # .iloc cuts the dataframe by index numbers (e.g., rows 0 to 20)
        page_data = df.iloc[start:end]
        
        # --- HANDLE NANs FOR JSON ---
        # JSON (JavaScript) hates "NaN" (Not a Number). It prefers "null".
        # We manually convert NaNs to Python 'None' (which becomes 'null' in JSON).
        data_list = []
        for _, row in page_data.iterrows():
            row_list = []
            for val in row:
                if pd.isna(val):
                    row_list.append(None)
                else:
                    row_list.append(val)
            data_list.append(row_list)
        
        return {
            "columns": df.columns.tolist(),
            "data": data_list,
            "total_rows": total_rows,
            "page": page,
            "page_size": page_size,
            # Calculate total pages needed
            "total_pages": (total_rows + page_size - 1) // page_size
        }
    except Exception as e:
        return {"error": str(e)}

# This endpoint updates a single cell in the dataset.
@app.post("/update_data")
async def update_data(
    file: UploadFile = File(...),
    row_index: str = Form(...),
    column: str = Form(...),
    value: str = Form(...)
):
    """Update a single cell in the dataset"""
    try:
        content = await file.read()
        df = pd.read_csv(io.BytesIO(content))
        
        row_index = int(row_index)
        
        # --- TYPE CONVERSION LOGIC ---
        # Check if the user is trying to empty a cell
        if value.strip().lower() in ["", "nan", "none", "null"]:
            # .at is the fastest way to access a single cell [row, col]
            df.at[row_index, column] = np.nan
        else:
            # If the column is numeric (int or float), try to convert the string input to a number.
            if df[column].dtype in ['int64', 'float64']:
                try:
                    df.at[row_index, column] = float(value)
                except ValueError:
                    # If conversion fails (e.g., user typed "apple" in a price column), keep it as text.
                    df.at[row_index, column] = value
            else:
                df.at[row_index, column] = value
        
        # Return updated CSV
        output = io.StringIO()
        df.to_csv(output, index=False)
        output.seek(0)
        
        return StreamingResponse(
            io.BytesIO(output.getvalue().encode()),
            media_type="text/csv",
            headers={"Content-Disposition": "attachment; filename=updated_data.csv"}
        )
    except Exception as e:
        return {"error": str(e)}